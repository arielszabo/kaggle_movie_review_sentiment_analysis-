{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import xgboost as xgb\n",
    "from tensorflow import keras\n",
    "from sklearn.base import TransformerMixin\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "### preprocessing:\n",
    "...\n",
    "\n",
    "##### labels:\n",
    "create hierarchical labels ->  first is it positive/negative/neutral and then  ...\n",
    "\n",
    "### models:\n",
    "* TFIDF + simple model (logreg = 0.61081 on test)\n",
    "* TFIDF + FC\n",
    "\n",
    "embedding - pre trained / trained on train data / pre trained + fine tuned\n",
    "* Doc2Vec + simple model\n",
    "* Word2Vec + avg/sum + simple model\n",
    "* FastText + avg/sum + simple model\n",
    "* Glove + avg/sum + simple model\n",
    "\n",
    "* best embedding + LSTM\n",
    "\n",
    "* ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join('raw_data', 'train.tsv'), sep='\\t', index_col='PhraseId')\n",
    "test = pd.read_csv(os.path.join('raw_data', 'test.tsv'), sep='\\t', index_col='PhraseId')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['len'] = train['Phrase'].str.split().str.len()\n",
    "train['Phrase'] = train['Phrase'].str.lower()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[:2, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(train['len'])\n",
    "# set(train[train['len'] <= 1]['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train[train['len'] <= 1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = list(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple TFIDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "X = train['Phrase']\n",
    "y = train['Sentiment']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=english_stopwords)),\n",
    "#     ('LSA', TruncatedSVD(n_components=400)),\n",
    "    ('logreg',LogisticRegression())\n",
    "])\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_train, y_train), model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple TFIDF + FC nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(train['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# set(re.findall(r'[^\\w\\s]', all_text))\n",
    "# re.sub(r'[^\\w\\s]', '', all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained W2V model + mean word weights + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(TransformerMixin):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = word2vec.wv.vector_size  #  if a text is empty we should return a vector of zeros with the same dimensionality as all the other vectors\n",
    "\n",
    "    @staticmethod\n",
    "    def my_toknizer(sentence):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        return nltk.tokenize.word_tokenize(sentence)\n",
    "\n",
    "    def get_embedding(self, sentence):\n",
    "        embeddings = []\n",
    "        for word in nltk.tokenize.word_tokenize(sentence.strip()):\n",
    "            if word in self.word2vec.wv.vocab:\n",
    "                word_value = self.word2vec.wv[word]\n",
    "            else:\n",
    "                word_value = np.zeros(self.dim)\n",
    "\n",
    "            embeddings.append(word_value)\n",
    "    \n",
    "        return embeddings\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        emb = [np.mean(self.get_embedding(sentence), axis=0) for sentence in X]\n",
    "        return np.stack(emb)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean(self.get_embedding(words), axis=0)\n",
    "            for words in X\n",
    "         ]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "pretrained_model_path = api.load('word2vec-google-news-300', return_path=True)\n",
    "pretrained_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = gensim.modelsWord2Vec(x_train.tolist(), size=128, window=5, min_count=1, workers=3)\n",
    "# w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "X = (train['Phrase']\n",
    "    .str.lower()\n",
    "    .str.replace(r'[^\\w\\s]', '')\n",
    "#     .str.replace('|'.join([r'[\\s\\b]{}[\\s\\b]'.format(w) for w in english_stopwords]), '') # stop words\n",
    "    )\n",
    "y = train['Sentiment']\n",
    "\n",
    "# Drop rows with lower number of words than 2:\n",
    "idxs = X[X.str.split().str.len() < 2].index\n",
    "X.drop(idxs, inplace=True)\n",
    "y.drop(idxs, inplace=True)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('words2vec', MeanEmbeddingVectorizer(w2v_model)),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_train, y_train), model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained W2V model + mean word weights + FC nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_toknizer(sentence):\n",
    "#         sentence = sentence.lower()\n",
    "#         sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "#         return nltk.tokenize.word_tokenize(sentence)\n",
    "\n",
    "# def get_embedding(word2vec, sentence):\n",
    "#     embeddings = []\n",
    "#     for word in my_toknizer(sentence.strip()):\n",
    "#         if word in word2vec.wv.vocab:\n",
    "#             word_value = word2vec.wv[word]\n",
    "#         else:\n",
    "#             word_value = np.zeros(word2vec.wv.vector_size)\n",
    "\n",
    "#         embeddings.append(word_value)\n",
    "    \n",
    "#     if embeddings:\n",
    "#         return embeddings\n",
    "#     else:\n",
    "#         return [0]*word2vec.wv.vector_size\n",
    "\n",
    "# for i in X:\n",
    "#     embs = get_embedding(w2v_model, i)\n",
    "#     emb = np.mean(embs, axis=0)\n",
    "#     if emb.shape != (300,):\n",
    "#         print(i, emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM without embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_word = 100\n",
    "max_sequence_length = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_vocab_word, lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x_train = tokenizer.texts_to_sequences(x_train)\n",
    "_x_train = keras.preprocessing.sequence.pad_sequences(_x_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "_x_test = tokenizer.texts_to_sequences(x_test)\n",
    "_x_test = keras.preprocessing.sequence.pad_sequences(_x_test, maxlen=max_sequence_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RnnBuild(max_words, embed_dim):\n",
    "    clf = keras.models.Sequential([\n",
    "        keras.layers.Embedding(max_words, output_dim=embed_dim),\n",
    "        keras.layers.Bidirectional(\n",
    "            keras.layers.LSTM(embed_dim)\n",
    "        ),\n",
    "        keras.layers.Dense(10, activation='relu'),\n",
    "        keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    clf.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Pipeline([\n",
    "    ('keras', keras.wrappers.scikit_learn.KerasClassifier(RnnBuild,\n",
    "                                                          max_words=max_vocab_word,\n",
    "                                                          embed_dim=128,\n",
    "                                                          epochs=10,\n",
    "                                                          batch_size=256,\n",
    "                                                          validation_split=0.1,\n",
    "                                                          callbacks=[\n",
    "                                                              keras.callbacks.EarlyStopping(patience=5),\n",
    "                                                              keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=0)\n",
    "                                                                    ],\n",
    "                                                          verbose=1))\n",
    "                                  \n",
    "])\n",
    "\n",
    "print(RnnBuild(max_words=max_vocab_word, embed_dim=128).summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.fit(_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.score(_x_train, y_train), rnn_model.score(_x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with the best embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
